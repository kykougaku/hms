{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Model 3"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/kanai/.pyenv/versions/3.11.8/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2024-02-17 17:46:31.171063: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2024-02-17 17:46:31.191177: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-17 17:46:31.191197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-17 17:46:31.191796: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-17 17:46:31.195449: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-02-17 17:46:31.725959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]},{"data":{"text/plain":["16"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["# Importing essential libraries\n","import gc\n","import os\n","import random\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from IPython.display import display\n","\n","# PyTorch for deep learning\n","import timm\n","import torch\n","import torch.nn as nn  \n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# torchvision for image processing and augmentation\n","import torchvision.transforms as transforms\n","\n","# Suppressing minor warnings to keep the output clean\n","warnings.filterwarnings('ignore', category=Warning)\n","\n","# Reclaim memory no longer in use.\n","gc.collect()"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class Config:\n","    seed=42\n","    image_transform=transforms.Resize((512, 512))\n","    num_folds=5\n","    \n","# Set the seed for reproducibility across multiple libraries\n","def set_seed(seed):\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    \n","set_seed(Config.seed)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/resnet34d/hms-train-resnet34d/resnet34d_fold0.pth'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model_resnet \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet34d\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, in_chans\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the trained weights from the corresponding file\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m model_resnet\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/resnet34d/hms-train-resnet34d/resnet34d_fold\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Append the loaded model to the models list\u001b[39;00m\n\u001b[1;32m     13\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model_resnet)\n","File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/resnet34d/hms-train-resnet34d/resnet34d_fold0.pth'"]}],"source":["# Load and store the trained models for each fold into a list\n","models = []\n","\n","# Load ResNet34d\n","for i in range(Config.num_folds):\n","    # Create the same model architecture as during training\n","    model_resnet = timm.create_model('resnet34d', pretrained=False, num_classes=6, in_chans=1)\n","    \n","    # Load the trained weights from the corresponding file\n","    model_resnet.load_state_dict(torch.load(f'/kaggle/input/resnet34d/hms-train-resnet34d/resnet34d_fold{i}.pth', map_location=torch.device('cpu')))\n","    \n","    # Append the loaded model to the models list\n","    models.append(model_resnet)\n","\n","# Reclaim memory no longer in use.\n","gc.collect()\n","\n","# Load EfficientNetB0\n","for j in range(Config.num_folds):\n","    # Create the same model architecture as during training\n","    model_effnet_b0 = timm.create_model('efficientnet_b0', pretrained=False, num_classes=6, in_chans=1)\n","    \n","    # Load the trained weights from the corresponding file\n","    model_effnet_b0.load_state_dict(torch.load(f'/kaggle/input/efficientnetb0/hms-train-efficientnetb0/efficientnet_b0_fold{j}.pth', map_location=torch.device('cpu')))\n","    \n","    # Append the loaded model to the models list\n","    models.append(model_effnet_b0)\n","    \n","# Reclaim memory no longer in use.\n","gc.collect()\n","    \n","# Load EfficientNetB1\n","for k in range(Config.num_folds):\n","    # Create the same model architecture as during training\n","    model_effnet_b1 = timm.create_model('efficientnet_b1', pretrained=False, num_classes=6, in_chans=1)\n","    \n","    # Load the trained weights from the corresponding file\n","    model_effnet_b1.load_state_dict(torch.load(f'/kaggle/input/efficientnetb1/hms-train-efficientnetb1/efficientnet_b1_fold{k}.pth', map_location=torch.device('cpu')))\n","    \n","    # Append the loaded model to the models list\n","    models.append(model_effnet_b1)\n","\n","# Reclaim memory no longer in use.\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load test data and sample submission dataframe\n","test_df = pd.read_csv(\"/kaggle/input/hms-harmful-brain-activity-classification/test.csv\")\n","submission = pd.read_csv(\"/kaggle/input/hms-harmful-brain-activity-classification/sample_submission.csv\")\n","\n","# Merge the submission dataframe with the test data on EEG IDs\n","submission = submission.merge(test_df, on='eeg_id', how='left')\n","\n","# Generate file paths for each spectrogram based on the EEG data in the submission dataframe\n","submission['path'] = submission['spectrogram_id'].apply(lambda x: f\"/kaggle/input/hms-harmful-brain-activity-classification/test_spectrograms/{x}.parquet\")\n","\n","# Display the first few rows of the submission dataframe\n","display(submission.head())\n","\n","# Reclaim memory no longer in use\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the weights for each model\n","weight_resnet34d = 0.25\n","weight_effnetb0 = 0.42\n","weight_effnetb1 = 0.33\n","\n","# Get file paths for test spectrograms\n","paths = submission['path'].values\n","test_predss = []\n","\n","# Generate predictions for each spectrogram using all models\n","for path in paths:\n","    eps = 1e-6\n","    # Read and preprocess spectrogram data\n","    data = pd.read_parquet(path)\n","    data = data.fillna(-1).values[:, 1:].T\n","    data = np.clip(data, np.exp(-6), np.exp(10))\n","    data = np.log(data)\n","    \n","    # Normalize the data\n","    data_mean = data.mean(axis=(0, 1))\n","    data_std = data.std(axis=(0, 1))\n","    data = (data - data_mean) / (data_std + eps)\n","    data_tensor = torch.unsqueeze(torch.Tensor(data), dim=0)\n","    data = Config.image_transform(data_tensor)\n","\n","    test_pred = []\n","    \n","    # Generate predictions using all models\n","    for model in models:\n","        model.eval()\n","        with torch.no_grad():\n","            pred = F.softmax(model(data.unsqueeze(0)))[0]\n","            pred = pred.detach().cpu().numpy()\n","        test_pred.append(pred)\n","        \n","    # Combine predictions from all models using weighted voting\n","    weighted_pred = weight_resnet34d * np.mean(test_pred[:Config.num_folds], axis=0) + \\\n","                     weight_effnetb0 * np.mean(test_pred[Config.num_folds:2*Config.num_folds], axis=0) + \\\n","                     weight_effnetb1 * np.mean(test_pred[2*Config.num_folds:], axis=0)\n","    \n","    test_predss.append(weighted_pred)\n","\n","# Convert the list of predictions to a NumPy array for further processing\n","test_predss = np.array(test_predss)\n","\n","# Reclaim memory no longer in use\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_predss"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7469972,"sourceId":59093,"sourceType":"competition"},{"datasetId":4297749,"sourceId":7392733,"sourceType":"datasetVersion"},{"datasetId":4297782,"sourceId":7392775,"sourceType":"datasetVersion"},{"datasetId":4304475,"sourceId":7402356,"sourceType":"datasetVersion"},{"datasetId":4304949,"sourceId":7403069,"sourceType":"datasetVersion"},{"datasetId":4334995,"sourceId":7447509,"sourceType":"datasetVersion"},{"datasetId":4336944,"sourceId":7450712,"sourceType":"datasetVersion"},{"datasetId":4413439,"sourceId":7581697,"sourceType":"datasetVersion"},{"datasetId":4413451,"sourceId":7581715,"sourceType":"datasetVersion"},{"datasetId":4413454,"sourceId":7581720,"sourceType":"datasetVersion"},{"sourceId":158958765,"sourceType":"kernelVersion"},{"sourceId":159333316,"sourceType":"kernelVersion"},{"sourceId":159396114,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":4}
